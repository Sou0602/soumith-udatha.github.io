<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soumith Udatha</title>
  
  <meta name="author" content="Soumith Udatha">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soumith Udatha</name>
              </p>
              <p>I am a second year Masters' Research Student at <a href="http://www.ri.cmu.edu">Carnegie Mellon University</a>, where I work on bridging the gaps for applying Reinforcement Learning to the systems in the real world. I finished my undergraduate studies in Mechanical Engineering from the Indian Institute of Technology, Bombay(IIT Bombay).
              </p>
<!--               <p>
                Previously I have interned at Indian Institute of Sciences, Bangalore, where I worked on <a href="https://stochlab.github.io/projects/Augmented%20Random%20Search.html">Imitation Learning</a>, <a href="https://stochlab.github.io/projects/Augmented%20Random%20Search.html">Reinforcement Learning and </a> 
                , <a href="https://stochlab.github.io/projects/slope.html"> Quadrupedal Locomotion</a> under the guidance of <a href="https://shishirny.github.io/">Shishir Kolathaya</a> and <a href="https://aml.ece.iisc.ac.in/index.php/Bharadwaj_Amrutur">Bharadwaj Amrutur</a>.
                I've received the <a href="https://www.iitm.ac.in/recognitions/student-awards">Srikanth Sundarajan Award</a>, <a href="https://www.iitm.ac.in/recognitions/student-awards">Ms Latha & Sampath Srinath Prize	</a>, 
                <a href="https://www.linkedin.com/feed/update/urn:li:activity:6495676923740553216">Mercedes Benz Drive Challenge Winner</a>,
                <a href="https://www.linkedin.com/posts/vivek-muralidharan-3536a416_fiction2science-f2s-freedomtoact-activity-6610738955337981952-o5y8">Continental Fiction2Science Challenge Winner</a>
                 and the FIITJEE Scholarship Award.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:soumith0602@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Soumith_Udatha_Oct.pdf">CV</a> &nbsp/&nbsp
<!--                 <a href="https://scholar.google.com/citations?user=xwNzYpgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/soumith-udatha/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/linkedInProfile.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/linkedin_cropped_12122.jfif" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in implementing Reinforcement Learning agents for applications in the real world. Making RL agents use data and representations in the world effectively is a higher level objective. Some of the applications I am currently looking at include RL agents for Autonomous Driving Scenarios, Goal-Conditioned RL for D4RL environments and CARLA Autonomous Driving challenges and in building co-operative agents for Human-Agent Collaboration tasks.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="roughT_stop()" onmouseover="roughT_start()" >
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <div class="two" id='roughT'><img src="data/norm_ocbc.gif" width="252" height="168"></div>
                <img src="data/norm_ocbc.gif" width="252" height="168">
              </div>
              <script type="text/javascript">
                function roughT_start() {
                  document.getElementById('roughT').style.opacity = "1";
                }

                function roughT_stop() {
                  document.getElementById('roughT').style.opacity = "0";
                }
                roughT_stop()
              </script>
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Imitating Past Successes can be Very Suboptimal</papertitle>
              <br>
              <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>,
              <strong>Soumith Udatha</strong>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>,
              <br>
              <em>36th Conference on Neural Information Processing Systems <a href="https://nips.cc/"> (NeurIPS 2022)</a>, New Orleans, USA </em>
              <br>
              <a href="https://arxiv.org/abs/2206.03378">arXiv</a> /
              <a href="https://github.com/ben-eysenbach/normalized-ocbc/blob/main/experiments.ipynb">code</a> /
<!--               <a href="https://github.com/StochLab/SlopedTerrainLinearPolicy">github</a> / -->
              <a href="https://www.youtube.com/watch?v=bXnnU68HYQA">video</a>
              <p>One seemingly-simple way of doing RL is to do imitation learning on successful trajectories, and prior methods like goal-conditioned imitation learning use this to great effect. 
                For draw a connection between these prior methods and reward maximization, showing that these prior methods do not quite correspond to reward maximization, and actually get be worse than doing nothing. 
                Our analysis suggests a simple fix.</p>
            </td>
          </tr> 

          <tr onmouseout="Roman_stop()" onmouseover="Roman_start()" >
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <div class="two" id='roman'><img src="data/ngsim_high_alpha.PNG" width="252" height="168"></div>
                <img src="data/ngsim_high_alpha.PNG" width="252" height="168">
              </div>
              <script type="text/javascript">
                function Roman_start() {
                  document.getElementById('roman').style.opacity = "1";
                }

                function Roman_stop() {
                  document.getElementById('roman').style.opacity = "0";
                }
                Roman_stop()
              </script>
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Safe Reinforcement Learning for Ramp Merging with Probabilistically Safe Control Barrier Functions
              </papertitle>
              <br>
              <strong>Soumith Udatha</strong>,
              <a href="https://www.andrew.cmu.edu/user/yiweilyu/">Yiwei Lyu</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/john-m-dolan/">John Dolan</a>,
              <br>
              <em>39th International Conference on Machine Learning <a href="https://icml.cc/Conferences/2022/Schedule?type=Workshop"> (ICML 2022)</a>,Baltimore, USA </em>
              <br>
              <a href="https://learn-to-race.org/workshop-sl4ad-icml2022/assets/papers/paper_5.pdf">paper</a> /
             <!-- <a href="https://stochlab.github.io/projects/Augmented%20Random%20Search.html">project page</a> /
              <a href="https://github.com/sashank-tirumala/stoch_robot_test2">github</a> /
              <a href="https://www.youtube.com/watch?v=LRbHetp0dcg&feature=youtu.be">video</a> / -->
              <p> Prior work has looked at applying reinforcement learning and imitation learning approaches to autonomous driving scenarios, but either the safety or the efficiency of the algorithm is compromised.With the use of control barrier functions embedded into the reinforcement learning policy, we arrive at safe policies to optimize the performance of the autonomous driving vehicle.
              </p>
            </td>
          </tr> 

          <tr onmouseout="ICRA_stop()" onmouseover="ICRA_start()" >
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICRA'><img src="data/Figure1_FI_PIC.png" width="252" height="168"></div>
                <img src="data/Figure1_FI_PIC.png" width="252" height="168">
              </div>
              <script type="text/javascript">
                function ICRA_start() {
                  document.getElementById('ICRA').style.opacity = "1";
                }

                function ICRA_stop() {
                  document.getElementById('ICRA').style.opacity = "0";
                }
                ICRA_stop()
              </script>
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>A novel foot interface versus voice for controlling a robotic endoscope holder</papertitle>
              <br>
              <a href="https://www.monash.edu/romi">Yanjun Yang</a>
              <strong>Soumith Udatha</strong>,
              <a href="https://www.monash.edu/romi">Dana Kulic</a>,
              <a href="https://lens.monash.edu/@elahe-abdi">Elahe Abdi</a>,
              <br>
              <em> 2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/9224440">paper</a> /
<!--               <a href="https://github.com/sashank-tirumala/pybRL">github</a> / 
              <a href="https://www.youtube.com/watch?v=3BQYX2vZdAg&t=92s">video</a> -->
              <p></p>
              <p>An ergonomocal and intuitive foot interface for assisted robotic surgeries. We propose a novel foot interface to give the
surgeon direct control over a robotic camera holder to replace
the human camera holding assistant. The foot interface should
allow the surgeon to control the camera while their hands are
occupied with the primary surgical task. It can control 4 degrees
of freedom of the cameraâ€™s pose at 2 speeds.
              </p>
            </td>
          </tr> 
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Want to make such a website? <a href="https://github.com/jonbarron/jonbarron_website">source code</a>,
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
